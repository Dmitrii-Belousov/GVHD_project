---
title: "Project_cGVHD"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(flextable)
library(gtsummary)
library(broom)
library(broom.helpers)
library(forestmodel)
library(ggResidpanel)
library(lmtest)
library(car)
library(emmeans)
library(patchwork)
library(corrplot) 
library(cluster)
library(ggpubr)
library(rstatix)
library(ggstatsplot)
library(ggpubr)
library(ggcorrplot)
library(pheatmap)
library(corrr)
library(ggfortify)
library(factoextra)
library(FactoMineR)
library(ggbiplot)
library(plotly)
library(ggrepel)
library(tidymodels)
library(embed)
library(viridis)
library(gridExtra)
library(glmnet)
library(Matrix)
library(reshape2)
library(NbClust)
library(renv)
```

#Чтение данных

```{r}

AI_Tcells <- read_csv(
  "../../raw_data/AI_Tcells_для ЕА - Sheet1.csv",
  locale = locale(decimal_mark = ","),  # указываем, что десятичный разделитель - запятая
  show_col_types = FALSE
)

glimpse(AI_Tcells)

sum(is.na(AI_Tcells$Abs_Value))

```

#Редактирование данных

```{r}
AI_Tcells_data <- AI_Tcells %>%
  mutate(Analysis_day = str_extract(Names, "^[^_]+")) %>% # Извлекаем часть до первого "_" 
  mutate(Сell_population = str_extract(Names, "(?<=\\/).*")) %>% # Извлекаем все после "/"
  select(ID, Analysis_day, Сell_population, everything()) %>% # Перемещаем извлеченные данные в начало
  separate_rows(Analysis_day, sep = ",") %>% # Создаем новые строки, если несколько значений разделены запятыми
  mutate(Analysis_day = str_trim(Analysis_day))


AI_Tcells_data <- AI_Tcells_data %>%
  mutate(cGVHD_present = ifelse(is.na(AI_Tcells_data$cGVHD_time), 0, 1))

AI_Tcells_data <- AI_Tcells_data %>%
  mutate(cGVHD_present = factor(cGVHD_present, levels = c(0, 1), labels = c("No", "Yes")))

AI_Tcells_data <- AI_Tcells_data %>%
  mutate(across(c(Analysis_day, Сell_population), ~ factor(.)))

glimpse(AI_Tcells_data)
```

```{r}

AI_Tcells_filtered <- AI_Tcells_data %>%
  mutate(
    Analysis_time = ifelse(
      str_detect(Analysis_day, "^ДЕНЬ\\sХРРТПХ\\s\\+180$"), cGVHD_time + 180,
      ifelse(
        str_detect(Analysis_day, "^ДЕНЬ\\sХРРТПХ\\s\\+90$"), cGVHD_time + 90,
        ifelse(
          str_detect(Analysis_day, "^ДЕНЬ\\sХРРТПХ\\s\\+60$"), cGVHD_time + 60,
          ifelse(
            str_detect(Analysis_day, "^ДЕНЬ\\sХРРТПХ\\s\\+30$"), cGVHD_time + 30,
            ifelse(
              str_detect(Analysis_day, "^ДЕНЬ\\sХРРТПХ$"), cGVHD_time,
              ifelse(
                str_detect(Analysis_day, "^\\+180$"), 180,
                ifelse(
                  str_detect(Analysis_day, "^\\+90$"), 90,
                  ifelse(
                    str_detect(Analysis_day, "^\\+60$"), 60,
                    ifelse(
                      str_detect(Analysis_day, "^\\+30$"), 30,
                      ifelse(
                        str_detect(Analysis_day, "^\\+365$"), 365, NA_real_
                      )))))))))))

 AI_Tcells_filtered <- AI_Tcells_filtered  %>%
  filter(Analysis_day != "+30" & Analysis_day != "+60")
 
  AI_Tcells_filtered <- AI_Tcells_filtered %>%
  filter(!(Analysis_time == 180 & !is.na(cGVHD_time) & cGVHD_time < 180))
                   
AI_Tcells_filtered <- AI_Tcells_filtered %>%
  select(ID, Time_OS, cGVHD_present, cGVHD_time, Analysis_day, Analysis_time, Сell_population, Abs_Value)

AI_Tcells_filtered <- AI_Tcells_filtered %>%
  arrange(ID, Analysis_time)

AI_Tcells_filtered <- AI_Tcells_filtered %>%
  mutate(across(c(Time_OS, cGVHD_time, Analysis_time), ~ factor(.)))

glimpse(AI_Tcells_filtered)

```

#Описательные статистики

```{r}

statistics <- list(
  
	      `_Количество субъектов` = ~length(.x) %>% as.character(),
	      `_Количество (есть данные)` = ~sum(!is.na(.x)) %>% as.character(),
	      `_Нет данных` = ~sum(is.na(.x)) %>% as.character(),
	      `_Ср. знач.` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", mean(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
	      `_Станд. отклон.` = ~ifelse(sum(!is.na(.x)) < 3, "Н/П*", sd(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
	      `_95% ДИ для среднего` = ~{
    n <- sum(!is.na(.x))
    ifelse(n < 3, "Н/П*", 
           paste0(round(mean(.x, na.rm = TRUE) - 1.96 * sd(.x, na.rm = TRUE) / sqrt(n), 2), " - ", round(mean(.x, na.rm = TRUE) + 1.96 * sd(.x, na.rm = TRUE) / sqrt(n), 2))
    )
  },
	      `_мин. - макс.` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", paste0(min(.x, na.rm = TRUE) %>% round(2), " - ", max(.x, na.rm = TRUE) %>% round(2))),
	      `_Медиана` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", median(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
	      `_Q1 - Q3` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", paste0(quantile(.x, 0.25, na.rm = TRUE) %>% round(2), " - ", quantile(.x, 0.75, na.rm = TRUE) %>% round(2)))
	)




#AI_Tcells_filtered %>%
 # select(`Analysis_day`, `Сell_population`, `cGVHD_present`, Abs_Value) %>%
  #group_by(`Analysis_day`, `Сell_population`, `cGVHD_present`) %>%
  #summarize(across(Abs_Value, statistics)) %>% # Применяем список statistics к Abs_Value
  #pivot_longer(cols = -c(`Analysis_day`, `Сell_population`, `cGVHD_present`)) %>%
  #separate(name, into = c("Variable", "Statistics"), sep = "__") %>%
  #pivot_wider(names_from = c(cGVHD_present, Statistics), values_from = value) %>%
  #arrange(Analysis_day, Сell_population) %>%
  #flextable() %>%
  #theme_box() %>%
  #align(align = "center", part = "all") %>%
  #merge_v(c("Analysis_day", "Сell_population"))



# Создаём функцию для t-теста
t_test_wrapper <- function(data, var) {
  # Проверяем достаточно ли данных в каждой группе
  n_yes <- sum(!is.na(data$Abs_Value[data$cGVHD_present == "Yes"]))
  n_no <- sum(!is.na(data$Abs_Value[data$cGVHD_present == "No"]))
  
  if(n_yes < 3 || n_no < 3) {
    return("Н/П*")
  }
  
  # Выполняем t-тест
  test_result <- try(
    t.test(Abs_Value ~ cGVHD_present, data = data)$p.value,
    silent = TRUE
  )
  
  if(inherits(test_result, "try-error")) {
    return("Ошибка")
  }
  
  return(as.character(round(test_result, 4)))
}

# Сначала соберем все p-values в отдельный датафрейм
p_values_df <- AI_Tcells_filtered %>%
  group_by(Analysis_day, Сell_population) %>%
  summarise(
    p_value = t_test_wrapper(cur_data(), Abs_Value),
    .groups = "drop"
  )

# Создаем вектор для хранения скорректированных p-values
p_values_df <- p_values_df %>%
  mutate(
    adj_p_value = p_value  # Сначала копируем исходные p-values
  )

# Находим индексы числовых p-values
numeric_indices <- !p_values_df$p_value %in% c("Н/П*", "Ошибка")

if(sum(numeric_indices) > 0) {
  # Конвертируем в числовой формат только валидные p-values
  numeric_p_values <- as.numeric(p_values_df$p_value[numeric_indices])
  
  # Применяем коррекцию только к числовым значениям
  adj_p_values <- p.adjust(numeric_p_values, method = "BH")
  
  # Обновляем только числовые значения в столбце adj_p_value
  p_values_df$adj_p_value[numeric_indices] <- as.character(round(adj_p_values, 4))
}

# Основной код для создания таблицы
result <- AI_Tcells_filtered %>%
  select(`Analysis_day`, `Сell_population`, `cGVHD_present`, Abs_Value) %>%
  group_by(`Analysis_day`, `Сell_population`, `cGVHD_present`) %>%
  summarize(across(Abs_Value, statistics), .groups = "drop") %>%
  pivot_longer(cols = -c(`Analysis_day`, `Сell_population`, `cGVHD_present`)) %>%
  separate(name, into = c("Variable", "Statistics"), sep = "__") %>%
  pivot_wider(names_from = c(cGVHD_present, Statistics), values_from = value) %>%
  # Присоединяем p-values
  left_join(p_values_df, by = c("Analysis_day", "Сell_population")) %>%
    rename(
    "t-test, p-value" = p_value,
    "t-test, adj p-value (BH)" = adj_p_value
  ) %>%
  # Создаём финальную таблицу
  arrange(Analysis_day, Сell_population) %>%
  flextable() %>%
  theme_box() %>%
  align(align = "center", part = "all") %>%
  merge_v(c("Analysis_day", "Сell_population"))

# Выводим таблицу
result

```

В таблицах столбцы No- для тех, у кого не развилось cGVHD, Yes - у кого развилось

## Описательная таблица только для дня анализа 90

```{r}
# Те же статистики 
statistics <- list(
  `_Количество субъектов` = ~length(.x) %>% as.character(),
  `_Количество (есть данные)` = ~sum(!is.na(.x)) %>% as.character(),
  `_Нет данных` = ~sum(is.na(.x)) %>% as.character(),
  `_Ср. знач.` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", mean(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
  `_Станд. отклон.` = ~ifelse(sum(!is.na(.x)) < 3, "Н/П*", sd(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
  `_95% ДИ для среднего` = ~{
    n <- sum(!is.na(.x))
    ifelse(n < 3, "Н/П*", 
           paste0(round(mean(.x, na.rm = TRUE) - 1.96 * sd(.x, na.rm = TRUE) / sqrt(n), 2), " - ", round(mean(.x, na.rm = TRUE) + 1.96 * sd(.x, na.rm = TRUE) / sqrt(n), 2))
    )
  },
  `_мин. - макс.` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", paste0(min(.x, na.rm = TRUE) %>% round(2), " - ", max(.x, na.rm = TRUE) %>% round(2))),
  `_Медиана` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", median(.x, na.rm = TRUE) %>% round(2) %>% as.character()),
  `_Q1 - Q3` = ~ifelse(sum(!is.na(.x)) == 0, "Н/П*", paste0(quantile(.x, 0.25, na.rm = TRUE) %>% round(2), " - ", quantile(.x, 0.75, na.rm = TRUE) %>% round(2)))
)

# Фильтруем данные только для +90 дней
AI_Tcells_90 <- AI_Tcells_filtered %>%
  filter(Analysis_day == "+90")

# Функция для t-теста 
t_test_wrapper <- function(data, var) {
  n_yes <- sum(!is.na(data$Abs_Value[data$cGVHD_present == "Yes"]))
  n_no <- sum(!is.na(data$Abs_Value[data$cGVHD_present == "No"]))
  
  if(n_yes < 3 || n_no < 3) {
    return("Н/П*")
  }
  
  test_result <- try(
    t.test(Abs_Value ~ cGVHD_present, data = data)$p.value,
    silent = TRUE
  )
  
  if(inherits(test_result, "try-error")) {
    return("Ошибка")
  }
  
  return(as.character(round(test_result, 4)))
}

# Собираем p-values только для данных 90-го дня
p_values_df_90 <- AI_Tcells_90 %>%
  group_by(Сell_population) %>%
  summarise(
    p_value = t_test_wrapper(cur_data(), Abs_Value),
    .groups = "drop"
  )

# Корректируем p-values
p_values_df_90 <- p_values_df_90 %>%
  mutate(
    adj_p_value = p_value
  )

# Находим индексы числовых p-values
numeric_indices <- !p_values_df_90$p_value %in% c("Н/П*", "Ошибка")

if(sum(numeric_indices) > 0) {
  numeric_p_values <- as.numeric(p_values_df_90$p_value[numeric_indices])
  adj_p_values <- p.adjust(numeric_p_values, method = "BH")
  p_values_df_90$adj_p_value[numeric_indices] <- as.character(round(adj_p_values, 4))
}

# Создаем финальную таблицу
result_90 <- AI_Tcells_90 %>%
  select(`Сell_population`, `cGVHD_present`, Abs_Value) %>%
  group_by(`Сell_population`, `cGVHD_present`) %>%
  summarize(across(Abs_Value, statistics), .groups = "drop") %>%
  pivot_longer(cols = -c(`Сell_population`, `cGVHD_present`)) %>%
  separate(name, into = c("Variable", "Statistics"), sep = "__") %>%
  pivot_wider(names_from = c(cGVHD_present, Statistics), values_from = value) %>%
  # Присоединяем p-values
  left_join(p_values_df_90, by = c("Сell_population")) %>%
  rename(
    "t-test, p-value" = p_value,
    "t-test, adj p-value (BH)" = adj_p_value
  ) %>%
  # Создаём финальную таблицу
  arrange(Сell_population) %>%
  flextable() %>%
  theme_box() %>%
  align(align = "center", part = "all")

# Выводим таблицу
result_90
```


#Графики без логарифмирования

## Без группировки, день 90

```{r}

# 2. Построение графиков 
analysis_days <- c("+90")  # Фиксируем день +90
cell_populations <- unique(AI_Tcells_filtered$Сell_population)

# Функция для создания одного графика
create_plot <- function(day, cell_pop, data) {
  data_subset <- data %>%
    filter(Analysis_day == day, Сell_population == cell_pop)

  if (nrow(data_subset) > 0) {
    p <- ggplot(data_subset, aes(x = cGVHD_present, y = Abs_Value, fill = cGVHD_present)) +
      geom_boxplot() +
      ggpubr::stat_compare_means(method = "t.test", label.y = 0.8 * max(data_subset$Abs_Value, na.rm = TRUE), label.x = 1.5) +
      labs(title = paste("Abs_Value by cGVHD_present\n(Analysis_day =", day, ", Cell Pop =", cell_pop, ")"),
           x = "cGVHD_present", y = "Abs_Value") +
      theme_bw() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1),
            plot.title = element_text(hjust = 0.5)) +
      scale_y_continuous(limits = function(x) {
        range_x <- max(x) - min(x)
        c(min(x) - 0.1 * range_x, max(x) + 0.1 * range_x)
      })
    
    return(p)
  } else {
    return(NULL) # Возвращаем NULL, если нет данных
  }
}

# Создание всех графиков
plots <- list()
for (cell_pop in cell_populations) {
  p <- create_plot("+90", cell_pop, AI_Tcells_filtered)
  if (!is.null(p)) {
    print(p)
    plots[[paste0("plot_90_", cell_pop)]] <- p
    #ggsave(paste0("plot_90_", cell_pop, ".png"), plot = p, width = 10, height = 7)
  }
}
```

# Графики с логарифмированием

```{r}


# 1. Добавление группирующей переменной для facet_wrap)
AI_Tcells_filtered_facet <- AI_Tcells_filtered %>%
  mutate(group = paste0("Group ", (as.numeric(Сell_population) -1) %/% 5 +1))


# 2. Построение графиков с использованием facet_wrap

analysis_days <- unique(AI_Tcells_filtered_facet$Analysis_day)
groups <- unique(AI_Tcells_filtered_facet$group)

for (day in analysis_days) {
  for (grp in groups) {
    data_subset <- AI_Tcells_filtered_facet %>%
      filter(Analysis_day == day, group == grp)

    if (nrow(data_subset) > 0) {
      # Логарифмирование значений Abs_Value
      data_subset <- data_subset %>%
        mutate(log_Abs_Value = log1p(Abs_Value)) # log1p для обработки нулей

       max_val <- max(data_subset$log_Abs_Value, na.rm = TRUE)

      p <- ggplot(data_subset, aes(x = cGVHD_present, y = log_Abs_Value, fill = cGVHD_present)) + 
        geom_boxplot() +
        ggpubr::stat_compare_means(method = "t.test", label.y = max_val - 0.1 * max_val) +
        labs(title = paste("log(Abs_Value + 1) by cGVHD_present\n(Analysis_day =", day, ", Group =", grp, ")"), 
             x = "cGVHD_present", y = "log(Abs_Value + 1)") + 
        theme_bw() +
        theme(axis.text.x = element_text(angle = 45, hjust = 1),
              plot.title = element_text(hjust = 0.5)) +
        facet_wrap(~Сell_population, scales = "free_x")

      print(p)
      #ggsave(paste0("plot_log_", day, "_group_", grp, ".png"), plot = p, width = 12, height = 8)
    }
  }
}

```

#GLM wide data

##Подготовка данных

```{r}

data_day90 <- filter(AI_Tcells_filtered, Analysis_day == "+90")

data_day180 <- filter(AI_Tcells_filtered, Analysis_day == "+180")


```

```{r}
data_wide_90 <- data_day90 %>%
  pivot_wider(names_from = Сell_population, values_from = Abs_Value)

data_90 <- data_wide_90 %>%
  select(-ID, -Time_OS, -cGVHD_time, -Analysis_day, -Analysis_time)


data_wide_180 <- data_day180 %>%
  pivot_wider(names_from = Сell_population, values_from = Abs_Value)

data_180 <- data_wide_180 %>%
  select(-ID, -Time_OS, -cGVHD_time, -Analysis_day, -Analysis_time)

```

## Первая модель, aliased coefficients

```{r}

model_90 <- glm(cGVHD_present ~ ., data = data_90, family = binomial)

# Модель для 180-го дня
model_180 <- glm(cGVHD_present ~ ., data = data_180, family = binomial)

summary(model_90)
summary(model_180)

```

## Диагностика

```#{r}
car::vif(model_90)
car::vif(model_180)
```

#Корреляции

```{r}

data_90_num <- data_90 %>%
  select(-cGVHD_present) 

data_90_cor <- cor(data_90_num)

corrplot(data_90_cor, method = "circle", type = "upper", tl.col = "black", tl.cex = 0.7)

corrplot(data_90_cor, method = 'number')

```

```{r}
correlation_matrix <- cor(model.matrix(model_90))

# Визуализация тепловой карты

melted_cor <- reshape2::melt(correlation_matrix)

ggplot(data = melted_cor, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "red") +  
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  
  labs(title = "Матрица корреляций", fill = "Коэффициент корреляции")
```


#LASSO применяет L1-регуляризацию, которая добавляет штрафной член к функции потерь. Этот штраф уменьшает коэффициенты некоторых предикторов до нуля, эффективно исключая их из модели.

##90

```{r}

# Подготовка данных: X - матрица предикторов, y - вектор отклика
X <- model.matrix(cGVHD_present ~ ., data = data_90)
y <- data_90$cGVHD_present

# Построение модели LASSO
lasso_model <- glmnet(X, y, family = "binomial", alpha = 1) # alpha = 1 для LASSO

# Выбор наилучшего lambda с помощью кросс-валидации
cv_lasso <- cv.glmnet(X, y, family = "binomial", alpha = 1)
best_lambda <- cv_lasso$lambda.min

# Модель с наилучшим lambda
lasso_model_best <- glmnet(X, y, family = "binomial", alpha = 1, lambda = best_lambda)

# Коэффициенты
coef(lasso_model_best)

```

##180

```{r}

# Подготовка данных: X - матрица предикторов, y - вектор отклика
X <- model.matrix(cGVHD_present ~ ., data = data_180)
y <- data_180$cGVHD_present

# Построение модели LASSO
lasso_model <- glmnet(X, y, family = "binomial", alpha = 1) # alpha = 1 для LASSO

# Выбор наилучшего lambda с помощью кросс-валидации
cv_lasso <- cv.glmnet(X, y, family = "binomial", alpha = 1)
best_lambda <- cv_lasso$lambda.min

# Модель с наилучшим lambda
lasso_model_best <- glmnet(X, y, family = "binomial", alpha = 1, lambda = best_lambda)

# Коэффициенты
coef(lasso_model_best)

```

##несколько попыток отобрать предикторы в модель, везде aliased coefficient

```{r}

model_90_filter <- glm(cGVHD_present ~ . - `4+ (IM STAT)` - `4+` - `4NV(СТАР2)` - `4NV` - `4ЕМ` - `4СМ(СТАР2)` - `4СМ` - `4ТREG(СТАР2)` - `4ТREG` - `4ТЕ(СТАР2)` - `4ТЕ` - `8+ (IM STAT)` - `8+` - `8EMTM` - `8NV(СТАР2)` - `8NV` - `8СМ(СТАР2)` - `8СМ` - `8ТЕ(СТАР2)` - `8ТЕ`, data = data_90, family = binomial)


summary(model_90_filter)

```

```{r}

model_90_filter_2 <- glm(cGVHD_present ~ `4+ (IM STAT)` + `4NV(СТАР2)` + `4NV` + `4NV_TH17` + `4NV_Th17TO1` + `4NV_TH2` + `4NV+226+` + `4NV+PD-1-TIGIT-` + `4NV+TIGIT+` + `4ЕМ_TH1` + `4ЕМ_TH17` + `4ЕМ_Th17TO1` + `4СМ` + `4СМ_TH22` + `4ТЕ+PD-1+TIGIT-` + `8+ (IM STAT)` + `8+` + `8+DR+` + `8+PD-1+` + `8+PD-1+TIGIT+` + `8+TIGIT+` + `8EMTM` + `8EMTM+226+` + `8EMTM+39+` + `8EMTM+DR+` + `8EMTM+PD-1+` +  `8EMTM+PD-1+TIGIT-` + `8EMTM+PD-1+TIGIT+` + `8EMTM+PD-1-TIGIT+` + `8EMTM+TIGIT+` + `8NV` + `8TREG` + `8ТЕ(СТАР2)` + `8ТЕ+226+` + `8ТЕ+DR+` + `8ТЕ+PD-1+` + `8ТЕ+PD-1+TIGIT+` + `8ТМ`, data = data_90, family = binomial)                          
```

```{r}

model_90_filter_2 <- glm(cGVHD_present ~ `4NV_TH17` + `4NV_TH2` + `4NV+226+` + `4NV+PD-1-TIGIT-` + `4ЕМ_TH17` + `4ЕМ_Th17TO1` + `8+DR+` + `8+PD-1+` + `8+PD-1+TIGIT+` + `8EMTM+39+` + `8EMTM+DR+` + `8EMTM+PD-1+` +  `8EMTM+PD-1+TIGIT-` + `8EMTM+TIGIT+` + `8NV` + `8TREG` + `8ТЕ+DR+` +   `8ТЕ+PD-1+TIGIT+`, data = data_90, family = binomial) 

summary(model_90_filter_2)

```

#Heat map

```{r}

# Создание корреляционной матрицы
correlation_matrix <- cor(data_90_num, method = "spearman")

# Иерархическая кластеризация
cor_hclust <- hclust(dist((1-correlation_matrix)), method = "complete")

# Создание тепловой карты 
pheatmap(
  correlation_matrix,
  cluster_rows = cor_hclust,
  cluster_cols = cor_hclust,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица",
  annotation_col = NULL,
  annotation_row = NULL,
  fontsize = 10
)
```

##кластеры

```{r}

# Функция для расчета SSE
wss <- function(k) {
  km <- kmeans(data_90_num, centers = k, nstart = 25)
  return(km$tot.withinss)
}

# Расчет SSE для k от 1 до 10
k.values <- 1:10
SSE <- sapply(k.values, wss)

# Построение графика
plot(k.values, SSE, type = "b", xlab = "Число кластеров", ylab = "Сумма квадратов ошибок (SSE)", 
     main = "Метод локтя")
lines(k.values, SSE, col = "red")
```

```{r}
library(cluster)

silhouette_scores <- sapply(2:10, function(k) {
  km <- kmeans(data_90_num, centers = k, nstart = 25)
  sil <- silhouette(km$cluster, dist(data_90_num))
  mean(sil[, 3])
})

# Построение графика
plot(2:10, silhouette_scores, type = "b", xlab = "Число кластеров", ylab = "Средний силуэтный коэффициент", 
     main = "Критерий силуэта")
lines(2:10, silhouette_scores, col = "red")
```

#Деление на 10 кластеров + отдельные матрицы

```{r}
clusters <- cutree(cor_hclust, k = 10)

table(clusters)
```

```{r}
cluster_1_vars <- names(which(clusters == 1))
cluster_2_vars <- names(which(clusters == 2))
cluster_3_vars <- names(which(clusters == 3))
cluster_4_vars <- names(which(clusters == 4))
cluster_5_vars <- names(which(clusters == 5))
cluster_6_vars <- names(which(clusters == 6))
cluster_7_vars <- names(which(clusters == 7))
cluster_8_vars <- names(which(clusters == 8))
cluster_9_vars <- names(which(clusters == 9))
cluster_10_vars <- names(which(clusters == 10))
```

```{r}
cluster_1_matrix <- correlation_matrix[cluster_1_vars, cluster_1_vars]

pheatmap(
  cluster_1_matrix,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для первого кластера"
)
```

```{r}
cluster_2_matrix <- correlation_matrix[cluster_2_vars, cluster_2_vars]

pheatmap(
  cluster_2_matrix,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для второго кластера"
)
```

```{r}
cluster_3_matrix <- correlation_matrix[cluster_3_vars, cluster_3_vars]

pheatmap(
  cluster_3_matrix,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для третьего кластера"
)
```

```{r}
cluster_4_matrix <- correlation_matrix[cluster_4_vars, cluster_4_vars]

pheatmap(
  cluster_4_matrix,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для четвертого кластера"
)
```

```{r}
cluster_5_matrix <- correlation_matrix[cluster_5_vars, cluster_5_vars]

pheatmap(
  cluster_5_matrix,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для пятого кластера"
)
```

```{r}
cluster_6_matrix <- correlation_matrix[cluster_6_vars, cluster_6_vars]

pheatmap(
  cluster_6_matrix,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для шестого кластера"
)
```

```{r}
cluster_7_matrix <- correlation_matrix[cluster_7_vars, cluster_7_vars]

pheatmap(
  cluster_7_matrix,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для седьмого кластера"
)
```

```{r}
cluster_8_matrix <- correlation_matrix[cluster_8_vars, cluster_8_vars]

pheatmap(
  cluster_8_matrix,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для восьмого кластера"
)
```

```{r}
cluster_9_matrix <- correlation_matrix[cluster_9_vars, cluster_9_vars]

pheatmap(
  cluster_9_matrix,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для девятого кластера"
)
```

```{r}
cluster_10_matrix <- correlation_matrix[cluster_10_vars, cluster_10_vars]

pheatmap(
  cluster_10_matrix,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для десятого кластера"
)
```

#Деление на 5 кластеров + корреляционные матрицы

```{r}
clusters_2 <- cutree(cor_hclust, k = 5)

table(clusters_2)
```

```{r}

cluster_1 <- names(which(clusters_2 == 1))
cluster_2 <- names(which(clusters_2 == 2))
cluster_3 <- names(which(clusters_2 == 3))
cluster_4 <- names(which(clusters_2 == 4))
cluster_5 <- names(which(clusters_2 == 5))

```

```{r}
cluster_matrix_1 <- correlation_matrix[cluster_1, cluster_1]

pheatmap(
  cluster_matrix_1,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для первого кластера"
)
```

```{r}
cluster_matrix_2 <- correlation_matrix[cluster_2, cluster_2]

pheatmap(
  cluster_matrix_2,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для второго кластера"
)
```

```{r}
cluster_matrix_3 <- correlation_matrix[cluster_3, cluster_3]

pheatmap(
  cluster_matrix_3,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для третьего кластера"
)
```

```{r}
cluster_matrix_4 <- correlation_matrix[cluster_4, cluster_4]

pheatmap(
  cluster_matrix_4,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для четвертого кластера"
)
```

```{r}
cluster_matrix_5 <- correlation_matrix[cluster_5, cluster_5]

pheatmap(
  cluster_matrix_5,
  cluster_rows = F,
  cluster_cols = F,
  color = colorRampPalette(c("blue", "white", "red"))(100),
  main = "Корреляционная матрица для пятого кластера"
)
```

# Модель по результатам heat map (отобранные предикторы), 90 день

```{r}

model_90_filter_finish <- glm(cGVHD_present ~ `4СМ_TH1` + `4_TH22`+ `TREG+PD-1+` + `4ЕМ_TH17` + `4ЕМ+39+` + `4ТREG_TH1` + `8ТЕ` + `4NV_TH2` + `TREG+226-TIGIT-` + `8NV+DR+` + `4NV+PD-1+TIGIT-` + `8EMTM+39+` + `8СМ+TIGIT+`, data = data_90, family = binomial) 


summary(model_90_filter_finish)

```

##Корреляции в model_90_filter_finish

```{r}

data_90_num_model <- data_90 %>%
  select(`4СМ_TH1`, `4_TH22`,`TREG+PD-1+`, `4ЕМ_TH17`, `4ЕМ+39+`, `4ТREG_TH1`, `8ТЕ`, `4NV_TH2`, `TREG+226-TIGIT-`, `8NV+DR+`, `4NV+PD-1+TIGIT-`, `8EMTM+39+`, `8СМ+TIGIT+`) 


data_90_num_model_cor <- cor(data_90_num_model)

corrplot(data_90_num_model_cor, method = "circle", type = "upper", tl.col = "black", tl.cex = 0.7)

corrplot(data_90_num_model_cor, method = 'number')

```

##Диагностика model_90_filter_finish 90 день

**Проверка формы зависимости логита от** $X$ на линейность:


```{r}
df_check <- model_90_filter_finish %>% broom::augment() %>% 
  dplyr::select(-starts_with("."), .fitted) %>% 
  dplyr::select(where(is.numeric)) %>% 
  pivot_longer(-.fitted) %>% 
  rename(logit = .fitted)
  
ggplot() +
  geom_point(aes(x = value, y = logit), df_check) +
  geom_smooth(aes(x = value, y = logit), df_check, color = "red", se = FALSE) +
  facet_wrap(~ name, scales = "free") +
  theme_bw()
```

**Мультиколлинеарность**:

```{r}
car::vif(model_90_filter_finish)

```

**Выбросы**

```{r, fig.width=8, fig.height=4}

resid_panel(model_90_filter_finish, plots = c("lev", "cookd"))
```

# Модель по результатам heat map (отобранные предикторы), 180 день

```{r}

model_180_filter_finish <- glm(cGVHD_present ~ `4СМ_TH1` + `4_TH22`+ `TREG+PD-1+` + `4ЕМ_TH17` + `4ЕМ+39+` + `4ТREG_TH1` + `8ТЕ` + `4NV_TH2` + `4NV_Th17TO1` + `8NV+DR+` + `4NV+PD-1+TIGIT-` + `8EMTM+39+` + `8СМ+TIGIT+`, data = data_180, family = binomial) 


summary(model_180_filter_finish)
```

```{r}
df_check <- model_180_filter_finish %>% broom::augment() %>% 
  dplyr::select(-starts_with("."), .fitted) %>% 
  dplyr::select(where(is.numeric)) %>% 
  pivot_longer(-.fitted) %>% 
  rename(logit = .fitted)
  
ggplot() +
  geom_point(aes(x = value, y = logit), df_check) +
  geom_smooth(aes(x = value, y = logit), df_check, color = "red", se = FALSE) +
  facet_wrap(~ name, scales = "free") +
  theme_bw()
```


#Множественный регрессионный анализ, поправка на множественное сравнение, Forrest plot и Vulcano plot

```{r}

# Копируем данные для переименования
data_90_copy <- data_90

# Функция для безопасных имен переменных
make_safe_names <- function(names) {
  # Заменяем недопустимые символы на '_'
  safe_names <- make.names(names, unique = TRUE)
  return(safe_names)
}

# Функция для выполнения регрессии и коррекции p-значений
perform_multiple_regression <- function(data) {
  # Находим числовые колонки, исключая целевую
  numeric_cols <- names(data)[sapply(data, is.numeric) & names(data) != "cGVHD_present"]
  
  # Создаем копию данных с безопасными именами
  data_safe <- data
  names(data_safe) <- make_safe_names(names(data))
  
  # Обновляем список числовых колонок с безопасными именами
  numeric_cols_safe <- make_safe_names(numeric_cols)
  
  # Список для хранения результатов
  regression_results <- list()
  
  # Цикл по всем числовым переменным
  for (i in seq_along(numeric_cols_safe)) {
    col <- numeric_cols_safe[i]
    original_col <- numeric_cols[i]
    
    formula <- as.formula(paste("cGVHD_present ~", col))
    model <- glm(formula, data = data_safe, family = binomial())
    
    # Извлечение результатов
    tidy_model <- tidy(model, conf.int = TRUE)
    tidy_model$original_variable <- original_col
    regression_results[[original_col]] <- tidy_model
  }
  
  # Коррекция p-значений методом Бенджамини-Хохберга
  p_values <- sapply(regression_results, function(x) x$p.value[2])
  adjusted_p_values <- p.adjust(p_values, method = "BH")
  
  # Обновление результатов скорректированными p-значениями
  for (i in seq_along(regression_results)) {
    regression_results[[names(regression_results)[i]]]$p.adj <- adjusted_p_values[i]
  }
  
  return(regression_results)
}

# Выполнение регрессионного анализа
results <- perform_multiple_regression(data_90_copy)

# Подготовка данных для Forest Plot
forest_data <- do.call(rbind, lapply(names(results), function(col) {
  res <- results[[col]]
  data.frame(
    variable = col,
    estimate = res$estimate[2],
    conf.low = res$conf.low[2],
    conf.high = res$conf.high[2],
    p.adj = res$p.adj[2]
  )
}))

# Forest Plot
forest_plot_1 <- ggplot(forest_data, aes(y = reorder(variable, estimate), x = estimate)) +
  geom_point(size = 3) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.2) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(title = "Forest Plot of Regression Coefficients", 
       x = "Coefficient Estimate", 
       y = "Variables") +
  theme(axis.text.x = element_blank()) + #убрали текст с вертикальной оси после coord_flip()
   scale_x_continuous(limits = c(-75, 25)) +
    coord_flip()

forest_plot_2 <- ggplot(forest_data, aes(y = reorder(variable, estimate), x = estimate, color = p.adj < 0.05)) +
  geom_point(size = 2) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = 0.1) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(title = "Forest Plot of Regression Coefficients (p-adjust)", 
       x = "Coefficient Estimate", 
       y = "Variables") +
  theme(axis.text.x = element_blank()) +
  scale_x_continuous(limits = c(-75, 50)) +
  coord_flip() +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black"))


# Volcano Plot 
volcano_plot_1 <- ggplot(forest_data, aes(x = estimate, y = -log10(p.adj))) +
  geom_point(aes(color = p.adj < 0.05)) +
  geom_text_repel(aes(label = variable),
                  box.padding = 0.5,
                  max.overlaps = Inf,
                  size = 2,
                  segment.size = 0.2,
                   segment.linetype = "solid",
                   segment.alpha = 0.5) + # Управляем толщиной линий
    scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black")) +
  theme_minimal() +
  labs(title = "Volcano Plot",
       x = "Coefficient Estimate",
       y = "-log10(Adjusted p-value)") +
  scale_x_continuous(limits = c(-20, 8)) +
   scale_y_continuous(limits = c(0, 0.18))

# Вывод результатов
print(forest_data)
forest_plot_1
forest_plot_2
volcano_plot_1

# Volcano Plot 
volcano_plot_2 <- ggplot(forest_data, aes(x = estimate, y = -log10(p.adj))) +
  # Добавляем прозрачность точкам через alpha
  geom_point(aes(color = p.adj < 0.05), alpha = 0.5) +
  # Добавляем условие для отображения подписей
  geom_text_repel(data = subset(forest_data, estimate < -5 | estimate > 1),
                  aes(label = variable),
                  box.padding = 0.5,
                  max.overlaps = Inf,
                  size = 4,
                  segment.size = 0.2,
                  segment.linetype = "solid",
                  segment.alpha = 0.5) +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "black")) +
  theme_minimal() +
  labs(title = "Volcano Plot",
       x = "Coefficient Estimate",
       y = "-log10(Adjusted p-value)") +
  scale_x_continuous(limits = c(-20, 8)) +
  scale_y_continuous(limits = c(0, 0.18))

volcano_plot_2
```

```{r}
volcano_plot_3 <- ggplot(forest_data, aes(x = estimate, y = -log10(p.adj))) +
  # Добавляем логику для трех цветов на основе estimate
  geom_point(aes(color = case_when(
    estimate < -5 ~ "below_threshold",
    estimate > 1 ~ "above_threshold",
    TRUE ~ "within_threshold"
  )),
  size = 3,  
  alpha = 0.7) + 
  
  # Добавляем подписи для выбросов
  geom_text_repel(data = subset(forest_data, estimate < -5 | estimate > 1),
                  aes(label = variable),
                  box.padding = 0.5,
                  max.overlaps = Inf,
                  size = 4,
                  segment.size = 0.2,
                  segment.linetype = "solid",
                  segment.alpha = 0.5) +
  
  # Применяем цветовую палитру 
  scale_color_manual(
    values = c(
      "below_threshold" = "#304289",  # Синий для точек ниже -5
      "above_threshold" = "#893056",  # Красный для точек выше 1
      "within_threshold" = "#898889"  # Серый для точек между порогами
    ),
    guide = "none"  # Убираем легенду
  ) +
  
  theme_minimal() +
  labs(title = "Volcano Plot",
       x = "Coefficient Estimate",
       y = "-log10(Adjusted p-value)") +
  scale_x_continuous(limits = c(-20, 8)) +
  scale_y_continuous(limits = c(0, 0.18))

volcano_plot_3
```






